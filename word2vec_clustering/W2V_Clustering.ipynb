{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Article Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\salat\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source_id</th>\n",
       "      <th>source_name</th>\n",
       "      <th>author</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "      <th>url_to_image</th>\n",
       "      <th>published_at</th>\n",
       "      <th>content</th>\n",
       "      <th>top_article</th>\n",
       "      <th>engagement_reaction_count</th>\n",
       "      <th>engagement_comment_count</th>\n",
       "      <th>engagement_share_count</th>\n",
       "      <th>engagement_comment_plugin_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>reuters</td>\n",
       "      <td>Reuters</td>\n",
       "      <td>Reuters Editorial</td>\n",
       "      <td>NTSB says Autopilot engaged in 2018 California...</td>\n",
       "      <td>The National Transportation Safety Board said ...</td>\n",
       "      <td>https://www.reuters.com/article/us-tesla-crash...</td>\n",
       "      <td>https://s4.reutersmedia.net/resources/r/?m=02&amp;...</td>\n",
       "      <td>2019-09-03T16:22:20Z</td>\n",
       "      <td>WASHINGTON (Reuters) - The National Transporta...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2528.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>the-irish-times</td>\n",
       "      <td>The Irish Times</td>\n",
       "      <td>Eoin Burke-Kennedy</td>\n",
       "      <td>Unemployment falls to post-crash low of 5.2%</td>\n",
       "      <td>Latest monthly figures reflect continued growt...</td>\n",
       "      <td>https://www.irishtimes.com/business/economy/un...</td>\n",
       "      <td>https://www.irishtimes.com/image-creator/?id=1...</td>\n",
       "      <td>2019-09-03T10:32:28Z</td>\n",
       "      <td>The States jobless rate fell to 5.2 per cent l...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        source_id      source_name              author  \\\n",
       "0           0          reuters          Reuters   Reuters Editorial   \n",
       "1           1  the-irish-times  The Irish Times  Eoin Burke-Kennedy   \n",
       "\n",
       "                                               title  \\\n",
       "0  NTSB says Autopilot engaged in 2018 California...   \n",
       "1       Unemployment falls to post-crash low of 5.2%   \n",
       "\n",
       "                                         description  \\\n",
       "0  The National Transportation Safety Board said ...   \n",
       "1  Latest monthly figures reflect continued growt...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.reuters.com/article/us-tesla-crash...   \n",
       "1  https://www.irishtimes.com/business/economy/un...   \n",
       "\n",
       "                                        url_to_image          published_at  \\\n",
       "0  https://s4.reutersmedia.net/resources/r/?m=02&...  2019-09-03T16:22:20Z   \n",
       "1  https://www.irishtimes.com/image-creator/?id=1...  2019-09-03T10:32:28Z   \n",
       "\n",
       "                                             content  top_article  \\\n",
       "0  WASHINGTON (Reuters) - The National Transporta...          0.0   \n",
       "1  The States jobless rate fell to 5.2 per cent l...          0.0   \n",
       "\n",
       "   engagement_reaction_count  engagement_comment_count  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        6.0                      10.0   \n",
       "\n",
       "   engagement_share_count  engagement_comment_plugin_count  \n",
       "0                  2528.0                              0.0  \n",
       "1                     2.0                              0.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_csv('articles_data.csv')\n",
    "df_raw.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe: (10437, 15)\n",
      "Pre-processed dataframe: (9882, 2)\n"
     ]
    }
   ],
   "source": [
    "custom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\n",
    "text_columns = [\"title\", \"description\", \"content\"]\n",
    "\n",
    "df = df_raw.copy()\n",
    "df[\"content\"] = df[\"content\"].fillna(\"\")\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Create text column based on title, description, and content\n",
    "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values and keep relevant columns\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    "\n",
    "print(f\"Original dataframe: {df_raw.shape}\")\n",
    "print(f\"Pre-processed dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(list_of_docs, model):\n",
    "    \"\"\"Generate vectors for list of documents using a Word Embedding\n",
    "\n",
    "    Args:\n",
    "        list_of_docs: List of documents\n",
    "        model: Gensim's Word Embedding\n",
    "\n",
    "    Returns:\n",
    "        List of document vectors\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for tokens in list_of_docs:\n",
    "        zero_vector = np.zeros(model.vector_size)\n",
    "        vectors = []\n",
    "        for token in tokens:\n",
    "            if token in model.wv:\n",
    "                try:\n",
    "                    vectors.append(model.wv[token])\n",
    "                except KeyError:\n",
    "                    continue\n",
    "        if vectors:\n",
    "            vectors = np.asarray(vectors)\n",
    "            avg_vec = vectors.mean(axis=0)\n",
    "            features.append(avg_vec)\n",
    "        else:\n",
    "            features.append(zero_vector)\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, workers=1, seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trumps', 0.9885429739952087),\n",
       " ('president', 0.9746477603912354),\n",
       " ('donald', 0.9274908304214478),\n",
       " ('ivanka', 0.9203856587409973),\n",
       " ('impeachment', 0.9195814728736877),\n",
       " ('pences', 0.9152246713638306),\n",
       " ('avlon', 0.914821207523346),\n",
       " ('biden', 0.9146034121513367),\n",
       " ('breitbart', 0.9144060015678406),\n",
       " ('vice', 0.906724750995636)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"trump\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9882, 100)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_docs = vectorize(tokenized_docs, model=model)\n",
    "len(vectorized_docs), len(vectorized_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbkmeans_clusters(\n",
    "\tX, \n",
    "    k, \n",
    "    mb, \n",
    "    print_silhouette_values, \n",
    "):\n",
    "    \"\"\"Generate clusters and print Silhouette metrics using MBKmeans\n",
    "\n",
    "    Args:\n",
    "        X: Matrix of features.\n",
    "        k: Number of clusters.\n",
    "        mb: Size of mini-batches.\n",
    "        print_silhouette_values: Print silhouette values per cluster.\n",
    "\n",
    "    Returns:\n",
    "        Trained clustering model and labels based on X.\n",
    "    \"\"\"\n",
    "    km = MiniBatchKMeans(n_clusters=k, batch_size=mb).fit(X)\n",
    "    print(f\"For n_clusters = {k}\")\n",
    "    print(f\"Silhouette coefficient: {silhouette_score(X, km.labels_):0.2f}\")\n",
    "    print(f\"Inertia:{km.inertia_}\")\n",
    "\n",
    "    if print_silhouette_values:\n",
    "        sample_silhouette_values = silhouette_samples(X, km.labels_)\n",
    "        print(f\"Silhouette values:\")\n",
    "        silhouette_values = []\n",
    "        for i in range(k):\n",
    "            cluster_silhouette_values = sample_silhouette_values[km.labels_ == i]\n",
    "            silhouette_values.append(\n",
    "                (\n",
    "                    i,\n",
    "                    cluster_silhouette_values.shape[0],\n",
    "                    cluster_silhouette_values.mean(),\n",
    "                    cluster_silhouette_values.min(),\n",
    "                    cluster_silhouette_values.max(),\n",
    "                )\n",
    "            )\n",
    "        silhouette_values = sorted(\n",
    "            silhouette_values, key=lambda tup: tup[2], reverse=True\n",
    "        )\n",
    "        for s in silhouette_values:\n",
    "            print(\n",
    "                f\"    Cluster {s[0]}: Size:{s[1]} | Avg:{s[2]:.2f} | Min:{s[3]:.2f} | Max: {s[4]:.2f}\"\n",
    "            )\n",
    "    return km, km.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 50\n",
      "Silhouette coefficient: 0.11\n",
      "Inertia:3571.503939450964\n",
      "Silhouette values:\n",
      "    Cluster 38: Size:55 | Avg:0.37 | Min:0.00 | Max: 0.55\n",
      "    Cluster 35: Size:75 | Avg:0.32 | Min:0.08 | Max: 0.50\n",
      "    Cluster 39: Size:95 | Avg:0.27 | Min:0.02 | Max: 0.46\n",
      "    Cluster 9: Size:93 | Avg:0.27 | Min:-0.00 | Max: 0.44\n",
      "    Cluster 25: Size:94 | Avg:0.25 | Min:-0.06 | Max: 0.48\n",
      "    Cluster 24: Size:56 | Avg:0.24 | Min:-0.03 | Max: 0.47\n",
      "    Cluster 6: Size:180 | Avg:0.24 | Min:-0.11 | Max: 0.51\n",
      "    Cluster 3: Size:152 | Avg:0.23 | Min:-0.06 | Max: 0.45\n",
      "    Cluster 46: Size:110 | Avg:0.22 | Min:-0.10 | Max: 0.47\n",
      "    Cluster 29: Size:34 | Avg:0.22 | Min:-0.05 | Max: 0.45\n",
      "    Cluster 42: Size:174 | Avg:0.22 | Min:-0.00 | Max: 0.39\n",
      "    Cluster 4: Size:108 | Avg:0.19 | Min:-0.13 | Max: 0.41\n",
      "    Cluster 12: Size:45 | Avg:0.19 | Min:0.02 | Max: 0.39\n",
      "    Cluster 36: Size:196 | Avg:0.17 | Min:-0.03 | Max: 0.37\n",
      "    Cluster 27: Size:84 | Avg:0.16 | Min:-0.05 | Max: 0.33\n",
      "    Cluster 19: Size:404 | Avg:0.15 | Min:-0.00 | Max: 0.33\n",
      "    Cluster 14: Size:578 | Avg:0.15 | Min:-0.04 | Max: 0.37\n",
      "    Cluster 18: Size:177 | Avg:0.14 | Min:-0.07 | Max: 0.39\n",
      "    Cluster 40: Size:136 | Avg:0.14 | Min:-0.09 | Max: 0.39\n",
      "    Cluster 23: Size:189 | Avg:0.14 | Min:0.01 | Max: 0.31\n",
      "    Cluster 41: Size:66 | Avg:0.14 | Min:-0.09 | Max: 0.35\n",
      "    Cluster 37: Size:127 | Avg:0.14 | Min:-0.01 | Max: 0.34\n",
      "    Cluster 21: Size:430 | Avg:0.13 | Min:-0.09 | Max: 0.37\n",
      "    Cluster 34: Size:244 | Avg:0.13 | Min:-0.13 | Max: 0.37\n",
      "    Cluster 17: Size:108 | Avg:0.13 | Min:-0.05 | Max: 0.34\n",
      "    Cluster 31: Size:477 | Avg:0.12 | Min:-0.10 | Max: 0.36\n",
      "    Cluster 45: Size:132 | Avg:0.11 | Min:-0.11 | Max: 0.33\n",
      "    Cluster 32: Size:201 | Avg:0.11 | Min:-0.08 | Max: 0.30\n",
      "    Cluster 2: Size:261 | Avg:0.10 | Min:-0.05 | Max: 0.26\n",
      "    Cluster 16: Size:98 | Avg:0.10 | Min:-0.11 | Max: 0.33\n",
      "    Cluster 47: Size:79 | Avg:0.10 | Min:-0.09 | Max: 0.34\n",
      "    Cluster 44: Size:214 | Avg:0.10 | Min:-0.06 | Max: 0.28\n",
      "    Cluster 26: Size:98 | Avg:0.08 | Min:-0.11 | Max: 0.29\n",
      "    Cluster 30: Size:226 | Avg:0.08 | Min:-0.11 | Max: 0.31\n",
      "    Cluster 20: Size:460 | Avg:0.08 | Min:-0.14 | Max: 0.30\n",
      "    Cluster 28: Size:380 | Avg:0.08 | Min:-0.14 | Max: 0.30\n",
      "    Cluster 5: Size:579 | Avg:0.08 | Min:-0.11 | Max: 0.29\n",
      "    Cluster 8: Size:137 | Avg:0.08 | Min:-0.08 | Max: 0.32\n",
      "    Cluster 48: Size:206 | Avg:0.08 | Min:-0.12 | Max: 0.29\n",
      "    Cluster 10: Size:444 | Avg:0.08 | Min:-0.05 | Max: 0.25\n",
      "    Cluster 13: Size:197 | Avg:0.07 | Min:-0.07 | Max: 0.23\n",
      "    Cluster 11: Size:73 | Avg:0.07 | Min:-0.12 | Max: 0.32\n",
      "    Cluster 43: Size:150 | Avg:0.07 | Min:-0.12 | Max: 0.31\n",
      "    Cluster 15: Size:131 | Avg:0.06 | Min:-0.09 | Max: 0.24\n",
      "    Cluster 22: Size:157 | Avg:0.05 | Min:-0.17 | Max: 0.30\n",
      "    Cluster 1: Size:301 | Avg:0.04 | Min:-0.17 | Max: 0.32\n",
      "    Cluster 33: Size:138 | Avg:0.03 | Min:-0.20 | Max: 0.26\n",
      "    Cluster 49: Size:281 | Avg:0.03 | Min:-0.16 | Max: 0.29\n",
      "    Cluster 0: Size:267 | Avg:-0.00 | Min:-0.18 | Max: 0.24\n",
      "    Cluster 7: Size:185 | Avg:-0.01 | Min:-0.22 | Max: 0.26\n"
     ]
    }
   ],
   "source": [
    "clustering, cluster_labels = mbkmeans_clusters(\n",
    "\tX=vectorized_docs,\n",
    "    k=50,\n",
    "    mb=500,\n",
    "    print_silhouette_values=True,\n",
    ")\n",
    "df_clusters = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"tokens\": [\" \".join(text) for text in tokenized_docs],\n",
    "    \"cluster\": cluster_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most representative terms per cluster (based on centroids):\n",
      "Cluster 0: december plunged total decided baker \n",
      "Cluster 1: wireless economists jpmorgan export sentiment \n",
      "Cluster 2: tweet praised insults prisoner criticized \n",
      "Cluster 3: delay mps referendum leo jo \n",
      "Cluster 4: serial trying shocked passenger contained \n",
      "Cluster 5: opens television orleans corps nationwide \n",
      "Cluster 6: charleston islands flooding ravaged carolinas \n",
      "Cluster 7: closed mission region missiles blamed \n",
      "Cluster 8: militants probable iraqi baghdad targeting \n",
      "Cluster 9: squad warm foursomes qualifying argentina \n",
      "Cluster 10: success usually fitness product edge \n",
      "Cluster 11: escalation responses assembly resume lift \n",
      "Cluster 12: putin rouhani vladimir presidents tayyip \n",
      "Cluster 13: aides ukrainian vizcarra congressional volodymyr \n",
      "Cluster 14: land keeping evil japanese highly \n",
      "Cluster 15: abused founding longtime resolution coup \n",
      "Cluster 16: stabbing fatally charged neighbor murdering \n",
      "Cluster 17: body murdering apartment whose neighbor \n",
      "Cluster 18: parties suspension cowett swinson impasse \n",
      "Cluster 19: prize formula raw tops bristol \n",
      "Cluster 20: minutes winner january bowl prize \n",
      "Cluster 21: freed funded crimes witness manhattan \n",
      "Cluster 22: appearances weekends decade another test \n",
      "Cluster 23: probably popular able ride netflix \n",
      "Cluster 24: cnns interested transcript ridiculed contributor \n",
      "Cluster 25: tanker ablaze yemen strikes arabian \n",
      "Cluster 26: patch tips delve access radar \n",
      "Cluster 27: cnnpolitics clinton barack pences complaint \n",
      "Cluster 28: 20th felicity sentences previous scene \n",
      "Cluster 29: centrifuges expressed merkel message introduce \n",
      "Cluster 30: environmental lawyers departments treasury reporters \n",
      "Cluster 31: planned rome kicked promised shareholder \n",
      "Cluster 32: buttigieg publicly hill kamala representative \n",
      "Cluster 33: phones version smartphone monthly starting \n",
      "Cluster 34: edinburgh tafida dealt beds jodie \n",
      "Cluster 35: category humberto landfall tropical floodwaters \n",
      "Cluster 36: emmanuel zelensky volodymyr whistleblowers impeaching \n",
      "Cluster 37: knife pleaded pleads indiana duluth \n",
      "Cluster 38: noaa sharpie claim assertions forecasters \n",
      "Cluster 39: proposals johnsons backstop pm benjamin \n",
      "Cluster 40: victims bomb kills dozens soldiers \n",
      "Cluster 41: popularity ai content elon commissions \n",
      "Cluster 42: benefits malone likes watching views \n",
      "Cluster 43: block discussions suspend warned lawmaker \n",
      "Cluster 44: daughter assaulting disappearance sexually cork \n",
      "Cluster 45: flows tariff lift slap markets \n",
      "Cluster 46: demonstrators clearing protesters unrest marched \n",
      "Cluster 47: impose gm delhi uaw geneva \n",
      "Cluster 48: tournament injury finished victory qualifying \n",
      "Cluster 49: bond results secured defender give \n"
     ]
    }
   ],
   "source": [
    "print(\"Most representative terms per cluster (based on centroids):\")\n",
    "for i in range(50):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = model.wv.most_similar(positive=[clustering.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Latest: Pakistan predicts bloodbath in Kashmir | Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news. | The Latest on the U.N. General Assembly's annual gathering of world leaders (all times local):\n",
      "11:34 a.m.\n",
      "Pakistani Prime Minister Imran Khan has denounced India's crackdown in Kashmir and warned of a \"bloodbath\" in the disputed region.\n",
      "Khan said Friday at… [+3543 chars]\n",
      "-------------\n",
      "Embattled Israeli PM fights for survival in do-over election | Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news. | A visibly frantic Prime Minister Benjamin Netanyahu is in the fight of his political life as the country heads to national elections for the second time this year.\n",
      "With Netanyahu locked in a razor tight race and facing the likelihood of criminal corruption c… [+7219 chars]\n",
      "-------------\n",
      "Israeli PM convenes Cabinet in West Bank ahead of election | Get breaking national and world news, broadcast video coverage, and exclusive interviews. Find the top news online at ABC news. | Israeli Prime Minister Benjamin Netanyahu is convening his final pre-election Cabinet meeting in a part of the West Bank he's vowed to annex if re-elected.\n",
      "His government is meeting Sunday at the Jordan Valley regional council rather than in Jerusalem. Natio… [+586 chars]\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "test_cluster = 29\n",
    "most_representative_docs = np.argsort(\n",
    "    np.linalg.norm(vectorized_docs - clustering.cluster_centers_[test_cluster], axis=1)\n",
    ")\n",
    "for d in most_representative_docs[:3]:\n",
    "    print(docs[d])\n",
    "    print(\"-------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e607abaa517475297adc4c5ce0e20c958febf53b6bace5f653445ec5f32eea0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
